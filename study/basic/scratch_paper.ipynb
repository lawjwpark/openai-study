{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7aa655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "OPENAI_API_KEY_PATH = \"/workspace/openai-study/openai_api_key.txt\"\n",
    "openai.api_key = open(OPENAI_API_KEY_PATH).readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b608199",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"\\\n",
    "공교롭게도 전날 Cie에서의 첫 입 역시 고구마 베이스에 전복장을 사용한 타르틀렛이라 재미있었다. 전날도 그렇고 생각보다 고구마의 단맛과 밀도 있는 질감이 쫄깃한 종류의 해산물을 잘 받쳐준다.\n",
    "민어 부레를 말려서 팝콘처럼 튀겨낸 뒤 파프리카 소스를 살짝 얹었다. 두께감이 있어 얇은 칩의 바삭함과는 다르게 에어리하다는 단어가 딱 들어맞는다. 고소함을 타고 코를 간질이는 카이옌 페퍼/피멘톤류의 풍성한 매콤함도 멋지다.\n",
    "민어 피시테리를 섞어 만든 버터와 위에 올린 말린 가다랑어로 악센트를 주었다. 잠봉 뵈르의 쾌락적인 탄수화물+지방 조합을 그대로 가져가면서 말린 생선의 풍미가 킥 역할을 제대로 한다. 작은 한 입이 압축된 맛을 내뿜는다는 점에서 염장과 건조의 매력을 잘 드러내주는 아뮤즈였다.\n",
    "이건 정말이지 식감이며 레이어며 밸런스며 너무 완벽해서 한 점 한 점 아주 천천히 아껴먹었다. 무화과 잎 오일의 경우 디저트에서 접해본 적이 있으나 일반 요리에서는 처음이었는데, 쌉싸래하지만 섬세한 특유의 향과 맛이 역시나 섬세한 재료인 전복과 무척 잘 어우러진다. 흥취에 슬슬 적응된다 싶을 즈음에 딜꽃이 환기를 시켜주고, 견과류를 넣어 비교적 캐릭터가 강한 아이올리를 곁들이면 폭발적인 맛이 터진다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3be3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\\\n",
    "전문성 기준:\n",
    "1. 음식에 대한 전문성\n",
    "2. 서비스에 대한 전문성\n",
    "3. 공간에 대한 전문성\n",
    "리뷰를 위 세 가지 전문성 기준으로 각각 10점 만점으로 평가해줘\n",
    "리뷰:\n",
    "{review}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda918a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전문성 기준:\n",
      "1. 음식에 대한 전문성\n",
      "2. 서비스에 대한 전문성\n",
      "3. 공간에 대한 전문성\n",
      "리뷰를 위 세 가지 전문성 기준으로 각각 10점 만점으로 평가해줘\n",
      "리뷰:\n",
      "공교롭게도 전날 Cie에서의 첫 입 역시 고구마 베이스에 전복장을 사용한 타르틀렛이라 재미있었다. 전날도 그렇고 생각보다 고구마의 단맛과 밀도 있는 질감이 쫄깃한 종류의 해산물을 잘 받쳐준다.\n",
      "민어 부레를 말려서 팝콘처럼 튀겨낸 뒤 파프리카 소스를 살짝 얹었다. 두께감이 있어 얇은 칩의 바삭함과는 다르게 에어리하다는 단어가 딱 들어맞는다. 고소함을 타고 코를 간질이는 카이옌 페퍼/피멘톤류의 풍성한 매콤함도 멋지다.\n",
      "민어 피시테리를 섞어 만든 버터와 위에 올린 말린 가다랑어로 악센트를 주었다. 잠봉 뵈르의 쾌락적인 탄수화물+지방 조합을 그대로 가져가면서 말린 생선의 풍미가 킥 역할을 제대로 한다. 작은 한 입이 압축된 맛을 내뿜는다는 점에서 염장과 건조의 매력을 잘 드러내주는 아뮤즈였다.\n",
      "이건 정말이지 식감이며 레이어며 밸런스며 너무 완벽해서 한 점 한 점 아주 천천히 아껴먹었다. 무화과 잎 오일의 경우 디저트에서 접해본 적이 있으나 일반 요리에서는 처음이었는데, 쌉싸래하지만 섬세한 특유의 향과 맛이 역시나 섬세한 재료인 전복과 무척 잘 어우러진다. 흥취에 슬슬 적응된다 싶을 즈음에 딜꽃이 환기를 시켜주고, 견과류를 넣어 비교적 캐릭터가 강한 아이올리를 곁들이면 폭발적인 맛이 터진다.\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "731dcf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"음식점에 대한 리뷰를 평가해줘\"},\n",
    "              {\"role\": \"user\", \"content\": prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3d7bcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7ZweLfxCDpBaqE7bbAt0PhvGWgsKR at 0x7f95bc051b30> JSON: {\n",
       "  \"id\": \"chatcmpl-7ZweLfxCDpBaqE7bbAt0PhvGWgsKR\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1688801285,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"\\uc74c\\uc2dd\\uc5d0 \\ub300\\ud55c \\uc804\\ubb38\\uc131: 9/10\\n\\ub9ac\\ubdf0\\uc5b4\\ub294 \\uc74c\\uc2dd\\uc5d0 \\ub300\\ud55c \\uc804\\ubb38\\uc131\\uc744 \\ubcf4\\uc5ec\\uc8fc\\uace0 \\uc788\\uc2b5\\ub2c8\\ub2e4. \\uace0\\uad6c\\ub9c8 \\ubca0\\uc774\\uc2a4\\uc5d0 \\uc804\\ubcf5\\uc7a5\\uc744 \\uc0ac\\uc6a9\\ud55c \\ud0c0\\ub974\\ud2c0\\ub81b\\uc758 \\uc870\\ud569\\uc744 \\uc798 \\uc774\\ud574\\ud558\\uace0, \\uace0\\uad6c\\ub9c8\\uc758 \\ub2e8\\ub9db\\uacfc \\ubc00\\ub3c4 \\uc788\\ub294 \\uc9c8\\uac10\\uc774 \\ud574\\uc0b0\\ubb3c\\uacfc \\uc798 \\uc5b4\\uc6b8\\ub9b0\\ub2e4\\ub294 \\uc810\\uc744 \\uc5b8\\uae09\\ud558\\uace0 \\uc788\\uc2b5\\ub2c8\\ub2e4.\\n\\n\\uc11c\\ube44\\uc2a4\\uc5d0 \\ub300\\ud55c \\uc804\\ubb38\\uc131: 8/10\\n\\uc11c\\ube44\\uc2a4\\uc5d0 \\ub300\\ud574\\uc11c\\ub294 \\uc9c1\\uc811\\uc801\\uc778 \\uc5b8\\uae09\\uc740 \\uc5c6\\uc9c0\\ub9cc, \\ubb38\\uc7a5 \\ud45c\\ud604\\uacfc \\uc0c1\\uc138\\ud55c \\uc2dd\\uac10, \\ub9db\\uc5d0 \\ub300\\ud55c \\uc124\\uba85\\uc744 \\ud1b5\\ud574 \\ub9ac\\ubdf0\\uc5b4\\uc758 \\uc74c\\uc2dd\\uc5d0 \\ub300\\ud55c \\uc804\\ubb38\\uc131\\uc744 \\uc5ff\\ubcfc \\uc218 \\uc788\\uc2b5\\ub2c8\\ub2e4.\\n\\n\\uacf5\\uac04\\uc5d0 \\ub300\\ud55c \\uc804\\ubb38\\uc131: 7/10\\n\\ub9ac\\ubdf0\\uc5b4\\ub294 \\uacf5\\uac04\\uc5d0 \\ub300\\ud55c \\uc804\\ubb38\\uc131\\uc744 \\ubcf4\\uc5ec\\uc8fc\\uc9c0\\ub294 \\uc54a\\uc2b5\\ub2c8\\ub2e4. \\ub9ac\\ubdf0\\uc5d0\\uc11c\\ub294 \\uc74c\\uc2dd \\uba54\\ub274\\uc640 \\ub9db\\uc5d0 \\ucd08\\uc810\\uc744 \\ub9de\\ucd94\\uace0 \\uc788\\uc73c\\uba70, \\uacf5\\uac04\\uc5d0 \\ub300\\ud55c \\uc5b8\\uae09\\uc740 \\uc5c6\\uc2b5\\ub2c8\\ub2e4.\\n\\n\\ucd1d\\ud3c9: \\uc804\\ubb38\\uc131 \\ud3c9\\uade0 8/10\\n\\ub9ac\\ubdf0\\uc5b4\\ub294 \\uc74c\\uc2dd\\uc5d0 \\ub300\\ud55c \\uc804\\ubb38\\uc131\\uc744 \\ubcf4\\uc5ec\\uc8fc\\ub294 \\uba74\\uc774 \\uc788\\uc73c\\uba70, \\ub9db\\uc5d0 \\ub300\\ud55c \\uc0c1\\uc138\\ud55c \\uc124\\uba85\\uacfc \\uc2dd\\uac10, \\uc870\\ud569\\ub4f1\\uc744 \\uc798 \\ud45c\\ud604\\ud558\\uace0 \\uc788\\uc2b5\\ub2c8\\ub2e4. \\uc138\\ubd80\\uc801\\uc778 \\uacf5\\uac04\\uc5d0 \\ub300\\ud55c \\ud3c9\\uac00\\ub294 \\uc5c6\\uc9c0\\ub9cc, \\uc804\\ubc18\\uc801\\uc73c\\ub85c \\uc804\\ubb38\\uc131\\uc774 \\ub192\\uc740 \\ub9ac\\ubdf0\\ub77c\\uace0 \\ud3c9\\uac00\\ud560 \\uc218 \\uc788\\uc2b5\\ub2c8\\ub2e4.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 792,\n",
       "    \"completion_tokens\": 418,\n",
       "    \"total_tokens\": 1210\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8db9e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "음식에 대한 전문성: 9/10\n",
      "리뷰어는 음식에 대한 전문성을 보여주고 있습니다. 고구마 베이스에 전복장을 사용한 타르틀렛의 조합을 잘 이해하고, 고구마의 단맛과 밀도 있는 질감이 해산물과 잘 어울린다는 점을 언급하고 있습니다.\n",
      "\n",
      "서비스에 대한 전문성: 8/10\n",
      "서비스에 대해서는 직접적인 언급은 없지만, 문장 표현과 상세한 식감, 맛에 대한 설명을 통해 리뷰어의 음식에 대한 전문성을 엿볼 수 있습니다.\n",
      "\n",
      "공간에 대한 전문성: 7/10\n",
      "리뷰어는 공간에 대한 전문성을 보여주지는 않습니다. 리뷰에서는 음식 메뉴와 맛에 초점을 맞추고 있으며, 공간에 대한 언급은 없습니다.\n",
      "\n",
      "총평: 전문성 평균 8/10\n",
      "리뷰어는 음식에 대한 전문성을 보여주는 면이 있으며, 맛에 대한 상세한 설명과 식감, 조합등을 잘 표현하고 있습니다. 세부적인 공간에 대한 평가는 없지만, 전반적으로 전문성이 높은 리뷰라고 평가할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(completion[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83050f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409e44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f2597a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hello, my name is John and I am a software engineer.\"\n",
    "model = \"text-davinci-003\"\n",
    "response = openai.Completion.create(engine=model, prompt=prompt, max_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ea774c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7ZqK0TEZRa4QBjgpbKzYr60mYXfJ5 at 0x7f6dcf6bc2f0> JSON: {\n",
       "  \"id\": \"cmpl-7ZqK0TEZRa4QBjgpbKzYr60mYXfJ5\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1688776960,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \" I have been writing software for over 5 years now. My experience is mainly within the JavaScript and Node.js ecosystems, but I also work with a variety of other web technologies. I have a deep passion for solving complex problems through elegant software design,\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 13,\n",
       "    \"completion_tokens\": 50,\n",
       "    \"total_tokens\": 63\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4be2efc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I have been writing software for over 5 years now. My experience is mainly within the JavaScript and Node.js ecosystems, but I also work with a variety of other web technologies. I have a deep passion for solving complex problems through elegant software design,'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0703fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d9c4fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "7/10. This review is quite well-written and comprehensive, and it does provide a good sense of the reviewer's opinion of the dish. However, it does lack a more professional tone and structure, and it could use some further editing.\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"\\\n",
    "공교롭게도 전날 Cie에서의 첫 입 역시 고구마 베이스에 전복장을 사용한 타르틀렛이라 재미있었다. 전날도 그렇고 생각보다 고구마의 단맛과 밀도 있는 질감이 쫄깃한 종류의 해산물을 잘 받쳐준다.\n",
    "민어 부레를 말려서 팝콘처럼 튀겨낸 뒤 파프리카 소스를 살짝 얹었다. 두께감이 있어 얇은 칩의 바삭함과는 다르게 에어리하다는 단어가 딱 들어맞는다. 고소함을 타고 코를 간질이는 카이옌 페퍼/피멘톤류의 풍성한 매콤함도 멋지다.\n",
    "민어 피시테리를 섞어 만든 버터와 위에 올린 말린 가다랑어로 악센트를 주었다. 잠봉 뵈르의 쾌락적인 탄수화물+지방 조합을 그대로 가져가면서 말린 생선의 풍미가 킥 역할을 제대로 한다. 작은 한 입이 압축된 맛을 내뿜는다는 점에서 염장과 건조의 매력을 잘 드러내주는 아뮤즈였다.\n",
    "이건 정말이지 식감이며 레이어며 밸런스며 너무 완벽해서 한 점 한 점 아주 천천히 아껴먹었다. 무화과 잎 오일의 경우 디저트에서 접해본 적이 있으나 일반 요리에서는 처음이었는데, 쌉싸래하지만 섬세한 특유의 향과 맛이 역시나 섬세한 재료인 전복과 무척 잘 어우러진다. 흥취에 슬슬 적응된다 싶을 즈음에 딜꽃이 환기를 시켜주고, 견과류를 넣어 비교적 캐릭터가 강한 아이올리를 곁들이면 폭발적인 맛이 터진다.\\\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "analyze the professionalism of the review out of 10:\n",
    "review: {review}\"\"\"\n",
    "\n",
    "print(response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb0ab48c",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b18fa001a439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m response = openai.Completion.create(\n\u001b[1;32m      2\u001b[0m     \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         )\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m                 ),\n\u001b[1;32m    706\u001b[0m                 \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             raise self.handle_error_response(\n\u001b[0;32m--> 764\u001b[0;31m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m             )\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo\",\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d629b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(animal):\n",
    "    return \"\"\"Suggest three names for an animal that is a superhero.\n",
    "\n",
    "Animal: Cat\n",
    "Names: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\n",
    "Animal: Dog\n",
    "Names: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\n",
    "Animal: {}\n",
    "Names:\"\"\".format(animal.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8719ac70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggest three names for an animal that is a superhero.\n",
      "\n",
      "Animal: Cat\n",
      "Names: Captain Sharpclaw, Agent Fluffball, The Incredible Feline\n",
      "Animal: Dog\n",
      "Names: Ruff the Protector, Wonder Canine, Sir Barks-a-Lot\n",
      "Animal: Unicorn\n",
      "Names:\n"
     ]
    }
   ],
   "source": [
    "print(generate_prompt(\"unicorn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f5420d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal = \"unicorn\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=generate_prompt(animal),\n",
    "  temperature=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac588476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7ZrQSI3OnZ0kwVS5SfpKcQt5o5DEv at 0x7f6dceaf6b30> JSON: {\n",
       "  \"id\": \"cmpl-7ZrQSI3OnZ0kwVS5SfpKcQt5o5DEv\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1688781204,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \" Captain Starhorn, Super Sparkle, The Majestic Mane\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 61,\n",
       "    \"completion_tokens\": 11,\n",
       "    \"total_tokens\": 72\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02b7871a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7ZuMwjLEorXyAn0NjI4pWQueFcYIr at 0x7f3666ba3170> JSON: {\n",
       "  \"id\": \"chatcmpl-7ZuMwjLEorXyAn0NjI4pWQueFcYIr\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1688792518,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 53,\n",
       "    \"completion_tokens\": 17,\n",
       "    \"total_tokens\": 70\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53ac6ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7ZuQ1E31qGcwEFfSQGTGTHs2gfjfS\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1688792709,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"The current weather in Boston, MA is sunny and windy with a temperature of 72 degrees.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 72,\n",
      "    \"completion_tokens\": 19,\n",
      "    \"total_tokens\": 91\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)\n",
    "\n",
    "\n",
    "def run_conversation():\n",
    "    # Step 1: send the conversation and available functions to GPT\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"]\n",
    "\n",
    "    # Step 2: check if GPT wanted to call a function\n",
    "    if response_message.get(\"function_call\"):\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        function_name = response_message[\"function_call\"][\"name\"]\n",
    "        fuction_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "        function_response = fuction_to_call(\n",
    "            location=function_args.get(\"location\"),\n",
    "            unit=function_args.get(\"unit\"),\n",
    "        )\n",
    "\n",
    "        # Step 4: send the info on the function call and function response to GPT\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )  # extend conversation with function response\n",
    "        second_response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "        return second_response\n",
    "\n",
    "\n",
    "print(run_conversation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16fec141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7ZuSmCrQ9Ej2gMlkQNE9jc4GwwwpI at 0x7f363dd1e230> JSON: {\n",
       "  \"id\": \"cmpl-7ZuSmCrQ9Ej2gMlkQNE9jc4GwwwpI\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1688792880,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\n\\nCool down with our creamy creations!\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 10,\n",
       "    \"completion_tokens\": 9,\n",
       "    \"total_tokens\": 19\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"Write a tagline for an ice cream shop.\"\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1002be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "test = [{'a': 'b'}, {'a': 'b'}, {'a': 'b'}]\n",
    "\n",
    "with open(\"data.qwer\", 'w') as f:\n",
    "    for item in test:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2743fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d9597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c609284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c03cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cefbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d2fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bdbd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c42b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3024e9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a665328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fe1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6a4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149cda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe614c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ae288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f033129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9792f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5014567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b714a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a620985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050150b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
